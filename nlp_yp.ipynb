{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "633e8f4c",
   "metadata": {},
   "source": [
    "# Предсказание сложности английского текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f51e92d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\1\\anaconda3\\lib\\site-packages (3.7)\n",
      "Requirement already satisfied: click in c:\\users\\1\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\1\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: joblib in c:\\users\\1\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\1\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\1\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c0b2518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textstat in c:\\users\\1\\anaconda3\\lib\\site-packages (0.7.3)\n",
      "Requirement already satisfied: pyphen in c:\\users\\1\\anaconda3\\lib\\site-packages (from textstat) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bddbb424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.27.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (21.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.10)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.21.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.64.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: setuptools in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (60.10.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\1\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.7.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\1\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\1\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\1\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\1\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\1\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\1\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\1\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.1)\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80f03e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\1\\anaconda3\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy) (60.10.0)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy) (8.1.10)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy) (2.4.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy) (1.10.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\1\\anaconda3\\lib\\site-packages (from spacy) (3.0.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\1\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.7.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\1\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\1\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\1\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\1\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\1\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
      "Requirement already satisfied: colorama in c:\\users\\1\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\1\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\1\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2e53c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from textstat import textstat\n",
    "import en_core_web_sm \n",
    "from sklearn.model_selection import train_test_split\n",
    "from catboost import CatBoostClassifier\n",
    "from collections import OrderedDict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "from warnings import simplefilter\n",
    "simplefilter(\"ignore\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0dbd89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e327da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\1\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9809a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d6c67d",
   "metadata": {},
   "source": [
    "### Начнем с загрузки данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e577bf89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level</th>\n",
       "      <th>Subtitles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B1</td>\n",
       "      <td>1\\n00:00:55,279 --&gt; 00:01:07,279\\n&lt;font color=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B1</td>\n",
       "      <td>1\\n00:01:54,281 --&gt; 00:01:55,698\\nHey!\\n\\n2\\n0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B2</td>\n",
       "      <td>1\\n00:00:15,089 --&gt; 00:00:21,229\\nResync: Xenz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B2</td>\n",
       "      <td>1\\n00:00:17,610 --&gt; 00:00:22,610\\n- &lt;i&gt;&lt;font c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2/A2+</td>\n",
       "      <td>1\\n00:00:27,240 --&gt; 00:00:30,879\\n&lt;i&gt;Oh, I com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>C2</td>\n",
       "      <td>ï»¿SeaWorld has suffered an 84% collapse in pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>C2</td>\n",
       "      <td>ï»¿They may not know who Steve Jobs was or eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>C2</td>\n",
       "      <td>ï»¿A long time ago, cinema audiences were tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>C2</td>\n",
       "      <td>ï»¿We may not yet be living in an age of flyin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>C2</td>\n",
       "      <td>ï»¿Setting aside epic disaster-movie moments s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1734 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Level                                          Subtitles\n",
       "0         B1  1\\n00:00:55,279 --> 00:01:07,279\\n<font color=...\n",
       "1         B1  1\\n00:01:54,281 --> 00:01:55,698\\nHey!\\n\\n2\\n0...\n",
       "2         B2  1\\n00:00:15,089 --> 00:00:21,229\\nResync: Xenz...\n",
       "3         B2  1\\n00:00:17,610 --> 00:00:22,610\\n- <i><font c...\n",
       "4     A2/A2+  1\\n00:00:27,240 --> 00:00:30,879\\n<i>Oh, I com...\n",
       "...      ...                                                ...\n",
       "1729      C2  ï»¿SeaWorld has suffered an 84% collapse in pr...\n",
       "1730      C2  ï»¿They may not know who Steve Jobs was or eve...\n",
       "1731      C2  ï»¿A long time ago, cinema audiences were tran...\n",
       "1732      C2  ï»¿We may not yet be living in an age of flyin...\n",
       "1733      C2  ï»¿Setting aside epic disaster-movie moments s...\n",
       "\n",
       "[1734 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levels = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']\n",
    "\n",
    "df1 = pd.DataFrame()  # Создаем пустой датафрейм\n",
    "\n",
    "\n",
    "folder_path = \"English_scores/Subtitles_all/Subtitles\"  # Замените на путь к нужной папке\n",
    "\n",
    "    # Получаем список всех файлов в папке\n",
    "file_names = os.listdir(folder_path)[1:]\n",
    "\n",
    "\n",
    "for filename in file_names:\n",
    "            with open(\"English_scores/Subtitles_all/Subtitles/\" + filename, 'r', encoding='latin-1') as f:  # Открываем файл по полному пути\n",
    "                file_content = f.read()  # Читаем содержимое файла\n",
    "\n",
    "                # Создаем новую строку в датасете с содержимым файла\n",
    "                new_row = pd.DataFrame({'Subtitles': [file_content],\n",
    "                                       'Movie':[filename]})\n",
    "                df1 = pd.concat([df1, new_row], ignore_index=True)\n",
    "\n",
    "def r(f):\n",
    "    f = f.replace('.srt', '')\n",
    "    return f\n",
    "df1['Movie'] = df1['Movie'].apply(r)\n",
    "\n",
    "df = pd.read_excel('English_scores/movies_labels.xlsx')\n",
    "df3 = pd.merge(df, df1, on =  'Movie')\n",
    "\n",
    "\n",
    "\n",
    "folder_path = \"English_scores/Subtitles_all/Subtitles\"  # Замените на путь к нужной папке\n",
    "for level in levels[1: -2]:\n",
    "        path = \"English_scores/Subtitles_all/\" + level\n",
    "        file_names = os.listdir(path)\n",
    "        for file in file_names:\n",
    "            with open(path + '/' + file, 'r', encoding='latin-1') as f:  # Открываем файл по полному пути\n",
    "                file_content = f.read()  # Читаем содержимое файла\n",
    "\n",
    "                # Создаем новую строку в датасете с содержимым файла\n",
    "                new_row = pd.DataFrame({'Subtitles': [file_content],\n",
    "                                       'Level':[level]})\n",
    "                df3= pd.concat([df3, new_row], ignore_index=True)\n",
    "df3 = df3.drop(['Movie', 'id'], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "for level in levels:\n",
    "        path = \"data\" + '/' + level\n",
    "        file_names = os.listdir(path)\n",
    "        for file in file_names:\n",
    "            with open(path + '/' + file, 'r', encoding='latin-1') as f:  # Открываем файл по полному пути\n",
    "                file_content = f.read()  # Читаем содержимое файла\n",
    "\n",
    "                # Создаем новую строку в датасете с содержимым файла\n",
    "                new_row = pd.DataFrame({'Subtitles': [file_content],\n",
    "                                       'Level':[level]})\n",
    "                df3= pd.concat([df3, new_row], ignore_index=True)\n",
    "df = df3.copy()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cc26d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad = pd.read_csv('cefr_leveled_texts.csv')\n",
    "\n",
    "ad.rename(columns={'text': 'Subtitles', 'label': 'Level'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "240c0345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-0df02eed7470>:1: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append(ad, ignore_index = True)\n"
     ]
    }
   ],
   "source": [
    "df = df.append(ad, ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db5f8720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level</th>\n",
       "      <th>Subtitles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B1</td>\n",
       "      <td>1\\n00:00:55,279 --&gt; 00:01:07,279\\n&lt;font color=...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B1</td>\n",
       "      <td>1\\n00:01:54,281 --&gt; 00:01:55,698\\nHey!\\n\\n2\\n0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B2</td>\n",
       "      <td>1\\n00:00:15,089 --&gt; 00:00:21,229\\nResync: Xenz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B2</td>\n",
       "      <td>1\\n00:00:17,610 --&gt; 00:00:22,610\\n- &lt;i&gt;&lt;font c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2/A2+</td>\n",
       "      <td>1\\n00:00:27,240 --&gt; 00:00:30,879\\n&lt;i&gt;Oh, I com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3223</th>\n",
       "      <td>C2</td>\n",
       "      <td>Light propagating in the vicinity of astrophys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3224</th>\n",
       "      <td>C2</td>\n",
       "      <td>Future of dentistry has become one of the most...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3225</th>\n",
       "      <td>C2</td>\n",
       "      <td>﻿The forests – and suburbs – of Europe are ech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3226</th>\n",
       "      <td>C2</td>\n",
       "      <td>Hedge funds are turning bullish on oil once ag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3227</th>\n",
       "      <td>C2</td>\n",
       "      <td>Without additional heating, radiative cooling ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3228 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Level                                          Subtitles\n",
       "0         B1  1\\n00:00:55,279 --> 00:01:07,279\\n<font color=...\n",
       "1         B1  1\\n00:01:54,281 --> 00:01:55,698\\nHey!\\n\\n2\\n0...\n",
       "2         B2  1\\n00:00:15,089 --> 00:00:21,229\\nResync: Xenz...\n",
       "3         B2  1\\n00:00:17,610 --> 00:00:22,610\\n- <i><font c...\n",
       "4     A2/A2+  1\\n00:00:27,240 --> 00:00:30,879\\n<i>Oh, I com...\n",
       "...      ...                                                ...\n",
       "3223      C2  Light propagating in the vicinity of astrophys...\n",
       "3224      C2  Future of dentistry has become one of the most...\n",
       "3225      C2  ﻿The forests – and suburbs – of Europe are ech...\n",
       "3226      C2  Hedge funds are turning bullish on oil once ag...\n",
       "3227      C2  Without additional heating, radiative cooling ...\n",
       "\n",
       "[3228 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e953768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B2            708\n",
       "A1            576\n",
       "A2            550\n",
       "C1            488\n",
       "B1            464\n",
       "C2            404\n",
       "A2/A2+         25\n",
       "B1, B2          8\n",
       "A2/A2+, B1      5\n",
       "Name: Level, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Level.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bb0223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c94659",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d56cccf4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6eafb7f1",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53159b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "POS_TAGS = [\n",
    "    \"ADJ\",\n",
    "    \"ADP\",\n",
    "    \"ADV\",\n",
    "    \"AUX\",\n",
    "    \"CONJ\",\n",
    "    \"CCONJ\",\n",
    "    \"DET\",\n",
    "    \"INTJ\",\n",
    "    \"NOUN\",\n",
    "    \"NUM\",\n",
    "    \"PART\",\n",
    "    \"PRON\",\n",
    "    \"PROPN\",\n",
    "    \"PUNCT\",\n",
    "    \"SCONJ\",\n",
    "    \"SYM\",\n",
    "    \"VERB\",\n",
    "    \"X\",\n",
    "    \"SPACE\",\n",
    "]\n",
    "\n",
    "\n",
    "def generate_features(data):\n",
    "    \"\"\"Generate features for a list of texts\n",
    "\n",
    "    Args:\n",
    "        data (list[str]): the dataset to be processed.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: the processed features.\n",
    "    \"\"\"\n",
    "    feature_data = []\n",
    "\n",
    "    for text in data:\n",
    "        \n",
    "        features = preprocess_text(text)\n",
    "        feature_data.append(features)\n",
    "    \n",
    "    return pd.DataFrame(feature_data)\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Takes a text, generate features, and returns as dict\n",
    "\n",
    "    Args:\n",
    "        text (str): the text to be preprocessed.\n",
    "\n",
    "    Returns:\n",
    "        dict: a dictionary of feature names with associated values\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    text = _simplify_punctuation(text)\n",
    "#     text = _remove_stop_words(text)\n",
    "#     text = _stem(text)\n",
    "\n",
    "    features = {\n",
    "#         \"words_count\": len(text.split()),\n",
    "#         \"symbols_count\": len(text),\n",
    "        \"flesch_reading_ease\": textstat.flesch_reading_ease(text),\n",
    "        \n",
    "#         \"textstat.syllable_count\":textstat.syllable_count(text),\n",
    "#         \"lexicon_count\":textstat.lexicon_count(text, removepunct=True),\n",
    "#         \"sentence_count\":textstat.sentence_count(text),\n",
    "#         \"polysyllabcount\":textstat.polysyllabcount(text),\n",
    "#         \"monosyllabcount\":textstat.monosyllabcount(text),\n",
    "        \n",
    "        \"spache_readability\":textstat.spache_readability(text),\n",
    "        \"mcalpine_eflaw\":textstat.mcalpine_eflaw(text),\n",
    "        \"smog_index\": textstat.smog_index(text),\n",
    "        \"flesch_kincaid_grade\": textstat.flesch_kincaid_grade(text),\n",
    "        \"coleman_liau_index\": textstat.coleman_liau_index(text),\n",
    "        \"automated_readability_index\": textstat.automated_readability_index(text),\n",
    "        \"dale_chall_readability_score\": textstat.dale_chall_readability_score(text),\n",
    "        \"difficult_words\": textstat.difficult_words(text) / len(text.split()),\n",
    "        \"linsear_write_formula\": textstat.linsear_write_formula(text),\n",
    "        \"gunning_fog\": textstat.gunning_fog(text),\n",
    "        \"text_standard\": textstat.text_standard(text, float_output=True),\n",
    "        \"mean_parse_tree_depth\": get_mean_parse_tree_depth(text),\n",
    "        \"mean_ents_per_sentence\": get_mean_ents_per_sentence(text),\n",
    "    }\n",
    "\n",
    "    features.update(get_mean_pos_tags(text))\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def _simplify_punctuation(text):\n",
    "        \n",
    "        text = text.replace('\\ufeff', '')\n",
    "        text = text.lower()\n",
    "        text = re.sub(r\"[,:;()\\-]\", \" \", text)  # Override commas, colons, etc to spaces/\n",
    "        text = re.sub(r\"[\\.!?]\", \".\", text)  # Change all terminators like ! and ? to \".\"\n",
    "        text = re.sub(r\"^\\s+\", \"\", text)  # Remove white space\n",
    "        text = re.sub(r\"[ ]*(\\n|\\r\\n|\\r)[ ]*\", \" \", text)  # Remove new lines\n",
    "        text = re.sub(r'\\d+:\\d+:\\d+,\\d+|\\d+|<[^>]*>|-|–|>|%|[’\\':;♪(){}\\[\\]]', '', text)  # Remove timecodes, digits, HTML tags, special characters, brackets, exclamation marks, question marks, colons, and other similar characters\n",
    "        text = re.sub(r\"([\\.])[\\. ]+\", \".\", text)  # Change all \"..\" to \".\"\n",
    "        text = re.sub(r\"[ ]*([\\.])\", \". \", text)  # Normalize all \".\"`\n",
    "        text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces\n",
    "        text = re.sub(r\"\\s+$\", \"\", text)  # Remove trailing spaces\n",
    "        text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text)\n",
    "        text = text.replace('\"', '')  # Remove smart quotes\n",
    "        text = ''.join(char for char in text if ord(char) < 128)\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    \n",
    "# def _remove_stop_words(text):\n",
    "#     # Tokenization - split text into individual words\n",
    "#     words = word_tokenize(text)\n",
    "\n",
    "#     # Remove stopwords\n",
    "#     stop_words = set(stopwords.words('english'))\n",
    "#     words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "#     # Join the processed words back into a single string\n",
    "#     processed_text = ' '.join(words)\n",
    "\n",
    "#     return processed_text\n",
    "\n",
    "\n",
    "# def _stem(text): \n",
    "#     stemmer = PorterStemmer()\n",
    "    \n",
    "#     # Tokenize the text into individual words\n",
    "#     words = word_tokenize(text)\n",
    "    \n",
    "#     # Apply stemming to each word and store the results in a list\n",
    "#     stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "#     # Join the stemmed words back into a single string\n",
    "#     stemmed_text = ' '.join(stemmed_words)\n",
    "    \n",
    "#     return stemmed_text\n",
    "\n",
    "def get_mean_parse_tree_depth(text):\n",
    "    \"\"\"Calculate the average depth of parse trees in the text\"\"\"\n",
    "    sentences = text.split(\".\")\n",
    "    depths = []\n",
    "    for doc in list(nlp.pipe(sentences)):\n",
    "        depths += _get_parse_tree_depths(doc)\n",
    "    return np.mean(depths)\n",
    "\n",
    "\n",
    "def _get_parse_tree_depths(doc):\n",
    "    return [_get_depth(token) for token in doc]\n",
    "\n",
    "\n",
    "def _get_depth(token, depth=0):\n",
    "    depths = [_get_depth(child, depth + 1) for child in token.children]\n",
    "    return max(depths) if len(depths) > 0 else depth\n",
    "\n",
    "\n",
    "def get_mean_pos_tags(text):\n",
    "    \"\"\"Calculate the mean for each type of POS tag in the text\"\"\"\n",
    "    sentences = text.split(\".\")\n",
    "    sentence_counts = _make_pos_tag_count_lists(sentences)\n",
    "    num_sentences = textstat.sentence_count(text)\n",
    "    mean_pos_tags = _calculate_mean_per_tag(sentence_counts, num_sentences)\n",
    "    return mean_pos_tags\n",
    "\n",
    "\n",
    "def _make_pos_tag_count_lists(sentences):\n",
    "    sentence_counts = {}\n",
    "    for doc in list(nlp.pipe(sentences)):\n",
    "        pos_counts = _get_pos_tag_counts(doc)\n",
    "        for key in pos_counts:\n",
    "            if key in sentence_counts:\n",
    "                sentence_counts[key].append(pos_counts[key])\n",
    "            else:\n",
    "                sentence_counts[key] = [pos_counts[key]]\n",
    "    return sentence_counts\n",
    "\n",
    "\n",
    "def _get_pos_tag_counts(doc):\n",
    "    pos_counts = {}\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    for tag in pos_tags:\n",
    "        if tag in pos_counts:\n",
    "            pos_counts[tag] += 1\n",
    "        else:\n",
    "            pos_counts[tag] = 1\n",
    "    return pos_counts\n",
    "\n",
    "\n",
    "def _calculate_mean_per_tag(counts, num_sentences):\n",
    "    mean_pos_tags = {f\"mean_{tag.lower()}\": 0 for tag in POS_TAGS}\n",
    "    for key in counts:\n",
    "        if len(counts[key]) < num_sentences:\n",
    "            counts[key] += [0] * (num_sentences - len(counts[key]))\n",
    "        mean_value = round(np.mean(counts[key]), 2)\n",
    "        mean_pos_tags[\"mean_\" + key.lower()] = mean_value\n",
    "    return mean_pos_tags\n",
    "\n",
    "\n",
    "def get_total_ents(text):\n",
    "    \"\"\"Get the total number of named entities in the text\"\"\"\n",
    "    return len(nlp(text).doc.ents)\n",
    "\n",
    "\n",
    "def get_mean_ents_per_sentence(text):\n",
    "    \"\"\"Calculate the average number of named entities per sentence in the text\"\"\"\n",
    "    return get_total_ents(text) / textstat.sentence_count(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa7abbb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>spache_readability</th>\n",
       "      <th>mcalpine_eflaw</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>dale_chall_readability_score</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>linsear_write_formula</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>text_standard</th>\n",
       "      <th>mean_parse_tree_depth</th>\n",
       "      <th>mean_ents_per_sentence</th>\n",
       "      <th>mean_adj</th>\n",
       "      <th>mean_adp</th>\n",
       "      <th>mean_adv</th>\n",
       "      <th>mean_aux</th>\n",
       "      <th>mean_conj</th>\n",
       "      <th>mean_cconj</th>\n",
       "      <th>mean_det</th>\n",
       "      <th>mean_intj</th>\n",
       "      <th>mean_noun</th>\n",
       "      <th>mean_num</th>\n",
       "      <th>mean_part</th>\n",
       "      <th>mean_pron</th>\n",
       "      <th>mean_propn</th>\n",
       "      <th>mean_punct</th>\n",
       "      <th>mean_sconj</th>\n",
       "      <th>mean_sym</th>\n",
       "      <th>mean_verb</th>\n",
       "      <th>mean_x</th>\n",
       "      <th>mean_space</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.24</td>\n",
       "      <td>2.65</td>\n",
       "      <td>10.7</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.06</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.21</td>\n",
       "      <td>0.087191</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>4.13</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.811132</td>\n",
       "      <td>0.171761</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97.30</td>\n",
       "      <td>2.52</td>\n",
       "      <td>11.5</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1.9</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.067151</td>\n",
       "      <td>3.153846</td>\n",
       "      <td>4.04</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.796791</td>\n",
       "      <td>0.175856</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98.92</td>\n",
       "      <td>2.26</td>\n",
       "      <td>9.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.61</td>\n",
       "      <td>1.6</td>\n",
       "      <td>5.59</td>\n",
       "      <td>0.060573</td>\n",
       "      <td>1.941176</td>\n",
       "      <td>3.24</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.753422</td>\n",
       "      <td>0.254371</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97.09</td>\n",
       "      <td>2.22</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.1</td>\n",
       "      <td>4.84</td>\n",
       "      <td>0.027238</td>\n",
       "      <td>5.222222</td>\n",
       "      <td>3.57</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.798168</td>\n",
       "      <td>0.139900</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97.09</td>\n",
       "      <td>2.53</td>\n",
       "      <td>11.7</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.7</td>\n",
       "      <td>4.02</td>\n",
       "      <td>2.4</td>\n",
       "      <td>5.74</td>\n",
       "      <td>0.063413</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>4.06</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.842644</td>\n",
       "      <td>0.262231</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3223</th>\n",
       "      <td>38.96</td>\n",
       "      <td>6.14</td>\n",
       "      <td>32.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>13.7</td>\n",
       "      <td>12.48</td>\n",
       "      <td>14.6</td>\n",
       "      <td>9.04</td>\n",
       "      <td>0.228169</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.76</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.780488</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.93</td>\n",
       "      <td>3.93</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3224</th>\n",
       "      <td>32.33</td>\n",
       "      <td>6.19</td>\n",
       "      <td>29.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>14.2</td>\n",
       "      <td>15.95</td>\n",
       "      <td>16.8</td>\n",
       "      <td>9.93</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>14.80</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.457429</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>2.77</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.04</td>\n",
       "      <td>6.89</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3225</th>\n",
       "      <td>45.29</td>\n",
       "      <td>6.28</td>\n",
       "      <td>34.8</td>\n",
       "      <td>13.4</td>\n",
       "      <td>13.4</td>\n",
       "      <td>13.23</td>\n",
       "      <td>16.2</td>\n",
       "      <td>9.19</td>\n",
       "      <td>0.210818</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>13.54</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.918954</td>\n",
       "      <td>1.535714</td>\n",
       "      <td>2.43</td>\n",
       "      <td>3.86</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0</td>\n",
       "      <td>1.07</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3226</th>\n",
       "      <td>61.26</td>\n",
       "      <td>5.19</td>\n",
       "      <td>24.9</td>\n",
       "      <td>12.2</td>\n",
       "      <td>9.3</td>\n",
       "      <td>11.26</td>\n",
       "      <td>11.3</td>\n",
       "      <td>9.40</td>\n",
       "      <td>0.202920</td>\n",
       "      <td>12.200000</td>\n",
       "      <td>10.59</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.230665</td>\n",
       "      <td>1.324324</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.18</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.11</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3227</th>\n",
       "      <td>36.22</td>\n",
       "      <td>7.11</td>\n",
       "      <td>35.4</td>\n",
       "      <td>16.4</td>\n",
       "      <td>14.8</td>\n",
       "      <td>13.00</td>\n",
       "      <td>16.3</td>\n",
       "      <td>10.94</td>\n",
       "      <td>0.296552</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>16.49</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.463816</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>3.27</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3228 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      flesch_reading_ease  spache_readability  mcalpine_eflaw  smog_index  \\\n",
       "0                   89.24                2.65            10.7         6.4   \n",
       "1                   97.30                2.52            11.5         6.2   \n",
       "2                   98.92                2.26             9.3         6.0   \n",
       "3                   97.09                2.22            12.0         5.7   \n",
       "4                   97.09                2.53            11.7         6.2   \n",
       "...                   ...                 ...             ...         ...   \n",
       "3223                38.96                6.14            32.7        14.6   \n",
       "3224                32.33                6.19            29.0        15.5   \n",
       "3225                45.29                6.28            34.8        13.4   \n",
       "3226                61.26                5.19            24.9        12.2   \n",
       "3227                36.22                7.11            35.4        16.4   \n",
       "\n",
       "      flesch_kincaid_grade  coleman_liau_index  automated_readability_index  \\\n",
       "0                      2.7                5.06                          3.2   \n",
       "1                      1.7                3.20                          1.9   \n",
       "2                      1.0                2.61                          1.6   \n",
       "3                      1.7                2.40                          1.1   \n",
       "4                      1.7                4.02                          2.4   \n",
       "...                    ...                 ...                          ...   \n",
       "3223                  13.7               12.48                         14.6   \n",
       "3224                  14.2               15.95                         16.8   \n",
       "3225                  13.4               13.23                         16.2   \n",
       "3226                   9.3               11.26                         11.3   \n",
       "3227                  14.8               13.00                         16.3   \n",
       "\n",
       "      dale_chall_readability_score  difficult_words  linsear_write_formula  \\\n",
       "0                             6.21         0.087191               3.750000   \n",
       "1                             5.75         0.067151               3.153846   \n",
       "2                             5.59         0.060573               1.941176   \n",
       "3                             4.84         0.027238               5.222222   \n",
       "4                             5.74         0.063413              58.000000   \n",
       "...                            ...              ...                    ...   \n",
       "3223                          9.04         0.228169              13.000000   \n",
       "3224                          9.93         0.263158              14.200000   \n",
       "3225                          9.19         0.210818              20.000000   \n",
       "3226                          9.40         0.202920              12.200000   \n",
       "3227                         10.94         0.296552              18.000000   \n",
       "\n",
       "      gunning_fog  text_standard  mean_parse_tree_depth  \\\n",
       "0            4.13            6.0               0.811132   \n",
       "1            4.04            2.0               0.796791   \n",
       "2            3.24            6.0               0.753422   \n",
       "3            3.57            2.0               0.798168   \n",
       "4            4.06            2.0               0.842644   \n",
       "...           ...            ...                    ...   \n",
       "3223        13.76           14.0               1.780488   \n",
       "3224        14.80           15.0               1.457429   \n",
       "3225        13.54           14.0               1.918954   \n",
       "3226        10.59           12.0               1.230665   \n",
       "3227        16.49           16.0               1.463816   \n",
       "\n",
       "      mean_ents_per_sentence  mean_adj  mean_adp  mean_adv  mean_aux  \\\n",
       "0                   0.171761      0.45      0.49      0.55      0.54   \n",
       "1                   0.175856      0.47      0.56      0.53      0.60   \n",
       "2                   0.254371      0.33      0.48      0.30      0.51   \n",
       "3                   0.139900      0.48      0.55      0.60      0.66   \n",
       "4                   0.262231      0.58      0.56      0.55      0.60   \n",
       "...                      ...       ...       ...       ...       ...   \n",
       "3223                0.000000      2.93      3.93      0.67      1.33   \n",
       "3224                0.461538      2.77      2.65      0.73      0.88   \n",
       "3225                1.535714      2.43      3.86      0.68      1.61   \n",
       "3226                1.324324      1.68      2.51      0.68      0.86   \n",
       "3227                0.090909      3.27      2.55      1.27      0.55   \n",
       "\n",
       "      mean_conj  mean_cconj  mean_det  mean_intj  mean_noun  mean_num  \\\n",
       "0             0        0.18      0.42       0.19       1.19      0.04   \n",
       "1             0        0.16      0.47       0.26       1.17      0.03   \n",
       "2             0        0.18      0.43       0.09       1.10      0.08   \n",
       "3             0        0.17      0.48       0.39       1.02      0.03   \n",
       "4             0        0.14      0.56       0.26       1.38      0.05   \n",
       "...         ...         ...       ...        ...        ...       ...   \n",
       "3223          0        0.73      3.60       0.00       6.40      0.00   \n",
       "3224          0        0.92      2.12       0.04       6.89      0.15   \n",
       "3225          0        1.07      2.04       0.00       7.21      0.14   \n",
       "3226          0        0.54      1.32       0.00       5.18      0.14   \n",
       "3227          0        1.55      2.27       0.00       9.36      0.00   \n",
       "\n",
       "      mean_part  mean_pron  mean_propn  mean_punct  mean_sconj  mean_sym  \\\n",
       "0          0.27       1.60        0.17        0.00        0.13      0.00   \n",
       "1          0.33       1.79        0.16        0.01        0.21      0.00   \n",
       "2          0.20       1.34        0.30        0.00        0.12      0.00   \n",
       "3          0.36       1.98        0.10        0.00        0.20      0.00   \n",
       "4          0.31       1.55        0.28        0.01        0.14      0.00   \n",
       "...         ...        ...         ...         ...         ...       ...   \n",
       "3223       0.20       0.60        0.07        0.00        0.47      0.00   \n",
       "3224       0.27       0.73        0.35        0.00        0.23      0.00   \n",
       "3225       0.54       1.36        1.32        0.00        0.64      0.00   \n",
       "3226       0.38       0.46        1.65        0.00        0.14      0.11   \n",
       "3227       0.36       0.91        0.55        0.00        0.64      0.00   \n",
       "\n",
       "      mean_verb  mean_x  mean_space  \n",
       "0          1.62    0.00        1.00  \n",
       "1          1.52    0.01        1.00  \n",
       "2          1.07    0.00        1.00  \n",
       "3          1.57    0.00        1.15  \n",
       "4          1.50    0.01        1.00  \n",
       "...         ...     ...         ...  \n",
       "3223       2.73    0.00        0.93  \n",
       "3224       2.65    0.00        1.00  \n",
       "3225       3.25    0.00        1.18  \n",
       "3226       2.32    0.16        1.06  \n",
       "3227       3.00    0.09        1.27  \n",
       "\n",
       "[3228 rows x 33 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd = generate_features(list(df.Subtitles.values))\n",
    "\n",
    "fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f8a071",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9eb7783e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fd[levels] = df['Subtitles'].apply(f, o_dict = level_dict).apply(pd.Series)\n",
    "\n",
    "fd['Level'] = df['Level']\n",
    "fd['Level'] = fd['Level'].replace('A2/A2+, B1', 'A2')\n",
    "fd['Level'] = fd['Level'].replace('B1, B2', 'B1')\n",
    "fd['Level'] = fd['Level'].replace('A2/A2+', 'A2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a010d92d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>spache_readability</th>\n",
       "      <th>mcalpine_eflaw</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>dale_chall_readability_score</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>linsear_write_formula</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>text_standard</th>\n",
       "      <th>mean_parse_tree_depth</th>\n",
       "      <th>mean_ents_per_sentence</th>\n",
       "      <th>mean_adj</th>\n",
       "      <th>mean_adp</th>\n",
       "      <th>mean_adv</th>\n",
       "      <th>mean_aux</th>\n",
       "      <th>mean_conj</th>\n",
       "      <th>mean_cconj</th>\n",
       "      <th>mean_det</th>\n",
       "      <th>mean_intj</th>\n",
       "      <th>mean_noun</th>\n",
       "      <th>mean_num</th>\n",
       "      <th>mean_part</th>\n",
       "      <th>mean_pron</th>\n",
       "      <th>mean_propn</th>\n",
       "      <th>mean_punct</th>\n",
       "      <th>mean_sconj</th>\n",
       "      <th>mean_sym</th>\n",
       "      <th>mean_verb</th>\n",
       "      <th>mean_x</th>\n",
       "      <th>mean_space</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89.24</td>\n",
       "      <td>2.65</td>\n",
       "      <td>10.7</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.06</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.21</td>\n",
       "      <td>0.087191</td>\n",
       "      <td>3.750000</td>\n",
       "      <td>4.13</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.811132</td>\n",
       "      <td>0.171761</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.42</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.19</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.60</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>97.30</td>\n",
       "      <td>2.52</td>\n",
       "      <td>11.5</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.7</td>\n",
       "      <td>3.20</td>\n",
       "      <td>1.9</td>\n",
       "      <td>5.75</td>\n",
       "      <td>0.067151</td>\n",
       "      <td>3.153846</td>\n",
       "      <td>4.04</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.796791</td>\n",
       "      <td>0.175856</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.17</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.33</td>\n",
       "      <td>1.79</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98.92</td>\n",
       "      <td>2.26</td>\n",
       "      <td>9.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.61</td>\n",
       "      <td>1.6</td>\n",
       "      <td>5.59</td>\n",
       "      <td>0.060573</td>\n",
       "      <td>1.941176</td>\n",
       "      <td>3.24</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.753422</td>\n",
       "      <td>0.254371</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.51</td>\n",
       "      <td>0</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.10</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97.09</td>\n",
       "      <td>2.22</td>\n",
       "      <td>12.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>1.7</td>\n",
       "      <td>2.40</td>\n",
       "      <td>1.1</td>\n",
       "      <td>4.84</td>\n",
       "      <td>0.027238</td>\n",
       "      <td>5.222222</td>\n",
       "      <td>3.57</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.798168</td>\n",
       "      <td>0.139900</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.66</td>\n",
       "      <td>0</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.02</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.36</td>\n",
       "      <td>1.98</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.15</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97.09</td>\n",
       "      <td>2.53</td>\n",
       "      <td>11.7</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.7</td>\n",
       "      <td>4.02</td>\n",
       "      <td>2.4</td>\n",
       "      <td>5.74</td>\n",
       "      <td>0.063413</td>\n",
       "      <td>58.000000</td>\n",
       "      <td>4.06</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.842644</td>\n",
       "      <td>0.262231</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.38</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.31</td>\n",
       "      <td>1.55</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.50</td>\n",
       "      <td>0.01</td>\n",
       "      <td>1.00</td>\n",
       "      <td>A2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3223</th>\n",
       "      <td>38.96</td>\n",
       "      <td>6.14</td>\n",
       "      <td>32.7</td>\n",
       "      <td>14.6</td>\n",
       "      <td>13.7</td>\n",
       "      <td>12.48</td>\n",
       "      <td>14.6</td>\n",
       "      <td>9.04</td>\n",
       "      <td>0.228169</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>13.76</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.780488</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.93</td>\n",
       "      <td>3.93</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1.33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.73</td>\n",
       "      <td>3.60</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.40</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.73</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.93</td>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3224</th>\n",
       "      <td>32.33</td>\n",
       "      <td>6.19</td>\n",
       "      <td>29.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>14.2</td>\n",
       "      <td>15.95</td>\n",
       "      <td>16.8</td>\n",
       "      <td>9.93</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>14.200000</td>\n",
       "      <td>14.80</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.457429</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>2.77</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0</td>\n",
       "      <td>0.92</td>\n",
       "      <td>2.12</td>\n",
       "      <td>0.04</td>\n",
       "      <td>6.89</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3225</th>\n",
       "      <td>45.29</td>\n",
       "      <td>6.28</td>\n",
       "      <td>34.8</td>\n",
       "      <td>13.4</td>\n",
       "      <td>13.4</td>\n",
       "      <td>13.23</td>\n",
       "      <td>16.2</td>\n",
       "      <td>9.19</td>\n",
       "      <td>0.210818</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>13.54</td>\n",
       "      <td>14.0</td>\n",
       "      <td>1.918954</td>\n",
       "      <td>1.535714</td>\n",
       "      <td>2.43</td>\n",
       "      <td>3.86</td>\n",
       "      <td>0.68</td>\n",
       "      <td>1.61</td>\n",
       "      <td>0</td>\n",
       "      <td>1.07</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.21</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.36</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.18</td>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3226</th>\n",
       "      <td>61.26</td>\n",
       "      <td>5.19</td>\n",
       "      <td>24.9</td>\n",
       "      <td>12.2</td>\n",
       "      <td>9.3</td>\n",
       "      <td>11.26</td>\n",
       "      <td>11.3</td>\n",
       "      <td>9.40</td>\n",
       "      <td>0.202920</td>\n",
       "      <td>12.200000</td>\n",
       "      <td>10.59</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.230665</td>\n",
       "      <td>1.324324</td>\n",
       "      <td>1.68</td>\n",
       "      <td>2.51</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.18</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.46</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.11</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.16</td>\n",
       "      <td>1.06</td>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3227</th>\n",
       "      <td>36.22</td>\n",
       "      <td>7.11</td>\n",
       "      <td>35.4</td>\n",
       "      <td>16.4</td>\n",
       "      <td>14.8</td>\n",
       "      <td>13.00</td>\n",
       "      <td>16.3</td>\n",
       "      <td>10.94</td>\n",
       "      <td>0.296552</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>16.49</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1.463816</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>3.27</td>\n",
       "      <td>2.55</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0</td>\n",
       "      <td>1.55</td>\n",
       "      <td>2.27</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.36</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>1.27</td>\n",
       "      <td>C2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3228 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      flesch_reading_ease  spache_readability  mcalpine_eflaw  smog_index  \\\n",
       "0                   89.24                2.65            10.7         6.4   \n",
       "1                   97.30                2.52            11.5         6.2   \n",
       "2                   98.92                2.26             9.3         6.0   \n",
       "3                   97.09                2.22            12.0         5.7   \n",
       "4                   97.09                2.53            11.7         6.2   \n",
       "...                   ...                 ...             ...         ...   \n",
       "3223                38.96                6.14            32.7        14.6   \n",
       "3224                32.33                6.19            29.0        15.5   \n",
       "3225                45.29                6.28            34.8        13.4   \n",
       "3226                61.26                5.19            24.9        12.2   \n",
       "3227                36.22                7.11            35.4        16.4   \n",
       "\n",
       "      flesch_kincaid_grade  coleman_liau_index  automated_readability_index  \\\n",
       "0                      2.7                5.06                          3.2   \n",
       "1                      1.7                3.20                          1.9   \n",
       "2                      1.0                2.61                          1.6   \n",
       "3                      1.7                2.40                          1.1   \n",
       "4                      1.7                4.02                          2.4   \n",
       "...                    ...                 ...                          ...   \n",
       "3223                  13.7               12.48                         14.6   \n",
       "3224                  14.2               15.95                         16.8   \n",
       "3225                  13.4               13.23                         16.2   \n",
       "3226                   9.3               11.26                         11.3   \n",
       "3227                  14.8               13.00                         16.3   \n",
       "\n",
       "      dale_chall_readability_score  difficult_words  linsear_write_formula  \\\n",
       "0                             6.21         0.087191               3.750000   \n",
       "1                             5.75         0.067151               3.153846   \n",
       "2                             5.59         0.060573               1.941176   \n",
       "3                             4.84         0.027238               5.222222   \n",
       "4                             5.74         0.063413              58.000000   \n",
       "...                            ...              ...                    ...   \n",
       "3223                          9.04         0.228169              13.000000   \n",
       "3224                          9.93         0.263158              14.200000   \n",
       "3225                          9.19         0.210818              20.000000   \n",
       "3226                          9.40         0.202920              12.200000   \n",
       "3227                         10.94         0.296552              18.000000   \n",
       "\n",
       "      gunning_fog  text_standard  mean_parse_tree_depth  \\\n",
       "0            4.13            6.0               0.811132   \n",
       "1            4.04            2.0               0.796791   \n",
       "2            3.24            6.0               0.753422   \n",
       "3            3.57            2.0               0.798168   \n",
       "4            4.06            2.0               0.842644   \n",
       "...           ...            ...                    ...   \n",
       "3223        13.76           14.0               1.780488   \n",
       "3224        14.80           15.0               1.457429   \n",
       "3225        13.54           14.0               1.918954   \n",
       "3226        10.59           12.0               1.230665   \n",
       "3227        16.49           16.0               1.463816   \n",
       "\n",
       "      mean_ents_per_sentence  mean_adj  mean_adp  mean_adv  mean_aux  \\\n",
       "0                   0.171761      0.45      0.49      0.55      0.54   \n",
       "1                   0.175856      0.47      0.56      0.53      0.60   \n",
       "2                   0.254371      0.33      0.48      0.30      0.51   \n",
       "3                   0.139900      0.48      0.55      0.60      0.66   \n",
       "4                   0.262231      0.58      0.56      0.55      0.60   \n",
       "...                      ...       ...       ...       ...       ...   \n",
       "3223                0.000000      2.93      3.93      0.67      1.33   \n",
       "3224                0.461538      2.77      2.65      0.73      0.88   \n",
       "3225                1.535714      2.43      3.86      0.68      1.61   \n",
       "3226                1.324324      1.68      2.51      0.68      0.86   \n",
       "3227                0.090909      3.27      2.55      1.27      0.55   \n",
       "\n",
       "      mean_conj  mean_cconj  mean_det  mean_intj  mean_noun  mean_num  \\\n",
       "0             0        0.18      0.42       0.19       1.19      0.04   \n",
       "1             0        0.16      0.47       0.26       1.17      0.03   \n",
       "2             0        0.18      0.43       0.09       1.10      0.08   \n",
       "3             0        0.17      0.48       0.39       1.02      0.03   \n",
       "4             0        0.14      0.56       0.26       1.38      0.05   \n",
       "...         ...         ...       ...        ...        ...       ...   \n",
       "3223          0        0.73      3.60       0.00       6.40      0.00   \n",
       "3224          0        0.92      2.12       0.04       6.89      0.15   \n",
       "3225          0        1.07      2.04       0.00       7.21      0.14   \n",
       "3226          0        0.54      1.32       0.00       5.18      0.14   \n",
       "3227          0        1.55      2.27       0.00       9.36      0.00   \n",
       "\n",
       "      mean_part  mean_pron  mean_propn  mean_punct  mean_sconj  mean_sym  \\\n",
       "0          0.27       1.60        0.17        0.00        0.13      0.00   \n",
       "1          0.33       1.79        0.16        0.01        0.21      0.00   \n",
       "2          0.20       1.34        0.30        0.00        0.12      0.00   \n",
       "3          0.36       1.98        0.10        0.00        0.20      0.00   \n",
       "4          0.31       1.55        0.28        0.01        0.14      0.00   \n",
       "...         ...        ...         ...         ...         ...       ...   \n",
       "3223       0.20       0.60        0.07        0.00        0.47      0.00   \n",
       "3224       0.27       0.73        0.35        0.00        0.23      0.00   \n",
       "3225       0.54       1.36        1.32        0.00        0.64      0.00   \n",
       "3226       0.38       0.46        1.65        0.00        0.14      0.11   \n",
       "3227       0.36       0.91        0.55        0.00        0.64      0.00   \n",
       "\n",
       "      mean_verb  mean_x  mean_space Level  \n",
       "0          1.62    0.00        1.00    B1  \n",
       "1          1.52    0.01        1.00    B1  \n",
       "2          1.07    0.00        1.00    B2  \n",
       "3          1.57    0.00        1.15    B2  \n",
       "4          1.50    0.01        1.00    A2  \n",
       "...         ...     ...         ...   ...  \n",
       "3223       2.73    0.00        0.93    C2  \n",
       "3224       2.65    0.00        1.00    C2  \n",
       "3225       3.25    0.00        1.18    C2  \n",
       "3226       2.32    0.16        1.06    C2  \n",
       "3227       3.00    0.09        1.27    C2  \n",
       "\n",
       "[3228 rows x 34 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c6b91a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fd.to_csv('datadata.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e94fff",
   "metadata": {},
   "source": [
    "## Получили признаки, на которых можем обучать модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81bffb2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "632765015a47406eb3684e9cc37d80f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.003678\n",
      "0:\tlearn: 1.7865941\ttotal: 191ms\tremaining: 2h 7m 16s\n",
      "10000:\tlearn: 0.2431344\ttotal: 5m 35s\tremaining: 16m 46s\n",
      "20000:\tlearn: 0.1119593\ttotal: 13m 11s\tremaining: 13m 11s\n",
      "30000:\tlearn: 0.0656743\ttotal: 21m 13s\tremaining: 7m 4s\n",
      "39999:\tlearn: 0.0453426\ttotal: 30m 40s\tremaining: 0us\n",
      "Accuracy: 0.9087\n"
     ]
    }
   ],
   "source": [
    "data = fd.copy().sample(frac = 1)\n",
    "X = data.drop('Level', axis=1)\n",
    "y = data['Level']\n",
    "\n",
    "# encoder = LabelEncoder()\n",
    "# y = encoder.fit_transform(y)\n",
    "\n",
    "# Step 2: Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# classes = np.unique(y_train)\n",
    "# weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "# class_weights = dict(zip(classes, weights))\n",
    "\n",
    "\n",
    "# Step 3: Create a CatBoost classifier and train it on the training data\n",
    "# You can customize the hyperparameters based on your specific use case\n",
    "catboost_model = CatBoostClassifier(iterations=40000,\n",
    "#                                     cat_features = cat_features,# Number of boosting iterations\n",
    "#                                     learning_rate=0.3,\n",
    "                                     # Learning rate for boosting\n",
    "#                                     depth=6,  # Depth of trees\n",
    "                                    loss_function='MultiClass',  # For multi-class classification\n",
    "#                                     eval_metric='MultiClass',  # Metric used for evaluation\n",
    "                                    random_seed=42  # Random seed for reproducibility\n",
    "#                                     class_weights = class_weights\n",
    "                                    )\n",
    "\n",
    "catboost_model.fit(X_train, y_train, verbose=10000, plot = True)  # Verbose setting shows progress every 100 iterations\n",
    "\n",
    "# Step 4: Evaluate the model on the test data\n",
    "accuracy = catboost_model.score(X_test, y_test)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82f26be",
   "metadata": {},
   "source": [
    "## Работа модели и ее результат"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c50fd8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Peter kicked his desk and stormed out of the classroom. Students in the rows next to him jumped, startled by the noise.\n",
    "\n",
    "'Hey!' shouted Mr Clark, the maths teacher, to Peter's back as he continued down the corridor. 'Detention!' he called after him. Peter pushed his hands in his pockets, head down and shoulders up around his neck as if he was cold. It was the third time this month that he'd walked out of class.\n",
    "\n",
    "***\n",
    "Maria had forgotten her sports kit for class again. Last week, she'd avoided PE with a note from her mum. Her mother hadn't written it, of course. Maria had forged her mum's writing and hoped the sports teacher wouldn't notice. It was the second time she'd 'forgotten' her kit and towel this term. She was running out of excuses.\n",
    "\n",
    "'Not good enough, Maria,' said her mother when the school called her to tell her Maria needed her kit. 'I put a clean towel on your bed this morning. I don't need the school calling and reminding me as if I'm a lazy mother. What's going on?'\n",
    "\n",
    "'Nothing, Mum,' mumbled Maria. \n",
    "\n",
    "'Look at me and speak clearly,' said her mum, annoyed. 'I can't understand a word you're saying.'\n",
    "\n",
    "Maria carried on looking down at the floor. At least her mum hadn't found the dress or the new trainers or the earrings – the presents guiltily hidden in her wardrobe. Then she would really be furious. And she'd start asking questions that Maria didn't want to answer.\n",
    "\n",
    "'Fine. But if you don't tell me, I can't help you.'\n",
    "\n",
    "***\n",
    "A week later, Peter and Maria were sitting outside the head teacher's office. Peter was staring at the wall angrily as if he was silently arguing with it. Maria glanced at him and then quickly back at her bitten fingernails and then back at him again. She didn't need to wonder why he was here because she was in his class. She'd never spoken to him before, but she'd seen him walk out of class. Today, she'd seen him kick the desk so hard it hit the window and cracked the glass.\n",
    "\n",
    "She wished she was here for something easy and straightforward like that. If she had damaged school property, it would be easy to apologise and promise never to do it again. She would pay for the damage and everyone would forget about it. Boys, especially teenage boys, were just aggressive sometimes, weren't they?\n",
    "\n",
    "She imagined the conversation Peter would have inside the head's office.\n",
    "\n",
    "'Why did you do that? Do you know how much furniture and windows cost?' Mr Hughes would say.\n",
    "\n",
    "'Sorry. I was just angry with the teacher. I have a bad temper sometimes,' Peter would reply.\n",
    "\n",
    "'It's not just me you need to apologise to. Here's the bill for the damage. Just don't do it again and the problem's solved, OK?'\n",
    "\n",
    "Easy.\n",
    "\n",
    "This simple solution of 'I'm sorry' and money wouldn't work for her. She was in trouble again about refusing to do PE. Her mother was on the way to the school, and Maria knew she was going to be asked questions that might lead to more questions. \n",
    "\n",
    "She watched Peter go in, slamming the door behind him. Even though she was sitting near the door, she couldn't hear any of their conversation. That meant they weren't shouting, so Peter was probably doing the apology part by now. After a few minutes, Peter left without looking at her. He didn't seem any less angry than before.\n",
    "\n",
    "****\n",
    "Maria didn't speak for eighteen minutes. She imagined herself floating above her mother's and Mr Hughes' heads, watching what was happening. Whenever they asked her a question, she lifted her shoulders in a silent shrug. It wasn't a great strategy, but it must have worked because they gave up eventually. It hadn't solved the real problem though. She could feel her phone vibrating inside her bag. She knew exactly who it was. Later she would have to answer his questions. Where had she been? Was she wearing the new dress? And detention wouldn't save her if he decided to wait outside school all night.\n",
    "\n",
    "***\n",
    "Peter and Maria were the only two students in detention on Monday. They were the only two on Tuesday too. They ignored each other on Monday, but exchanged nods on Tuesday. On Wednesday, Peter said, 'Hello again', when they arrived and Maria said, 'See you tomorrow' when they left. They both managed a smile at that.\n",
    "\n",
    "On Thursday, Maria noticed bruises on Peter's stomach when he pulled his school bag over his head. They looked like they'd been there a while. He saw her staring and she looked away, pretending she hadn't seen anything. He pulled his shirt back down and his cheeks coloured. They didn't exchange words that day, but Maria felt as if, somehow, a kind of conversation had taken place anyway.\n",
    "\n",
    "Maria wondered if Peter had someone he was afraid of. Someone who was nice at first and then later was like a different person. Someone who made him do things that made him feel uncomfortable. Someone who kept secrets and told him he had better be good at keeping secrets.\n",
    "\n",
    "On Friday, she didn't bother putting her phone on silent. So what if the detention teacher saw the messages? Maybe that would be the beginning of the end of the nightmare. But the phone didn't make a sound. She deliberately pushed her sleeves up to above her elbows. She'd been so careful to hide her arms, but now the bruises were clearly on display. She remembered what her mother had said about helping her. She didn't say anything, but she hoped Peter would see them and hear the silent question. 'You too?'\n",
    "\n",
    "***\n",
    "Whatever Peter thought when he saw her arms, Maria didn't know because he left detention without a word. That night she waited until her mother was busy in the kitchen to talk to her.\n",
    "\n",
    "She didn't know how to start. She practised different sentences in her head but she couldn't get the first word out of her mouth. 'Mum?' she said finally.\n",
    "\n",
    "Her mother didn't look up from the vegetables she was preparing. 'Mmmm?'\n",
    "\n",
    "'There's this boy at school and ...' Maria stopped. 'I saw something.'\n",
    "\n",
    "'Saw what?' She had her mother's attention now.\n",
    "\n",
    "'Something he didn't want me to see. A secret.'\n",
    "\n",
    "'What kind of secret?' her mother said carefully.\n",
    "\n",
    "'A bad secret – like I think someone is hurting him,' Maria said. But what if you tell someone and everyone thinks it's your fault? And what if you get someone in trouble and they get angry?' \n",
    "\n",
    "'Bad secrets are only bad until you tell someone,' her mother said. 'This boy needs to tell someone. 'But he has to choose the right person. A person who isn't going to say it's his fault, who's going to help.'\n",
    "\n",
    "'Who is the right person?' asked Maria.\n",
    "\n",
    "'An adult,' said her mother. 'One he trusts.'\n",
    "\n",
    "Maria took a deep breath. She took her phone out of her bag and opened up the messages. The first word still wouldn't come. 'Mum?' she said finally. 'I have to tell you something …'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925fd143",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "541315f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = generate_features([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1f25d725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "      <th>spache_readability</th>\n",
       "      <th>mcalpine_eflaw</th>\n",
       "      <th>smog_index</th>\n",
       "      <th>flesch_kincaid_grade</th>\n",
       "      <th>coleman_liau_index</th>\n",
       "      <th>automated_readability_index</th>\n",
       "      <th>dale_chall_readability_score</th>\n",
       "      <th>difficult_words</th>\n",
       "      <th>linsear_write_formula</th>\n",
       "      <th>gunning_fog</th>\n",
       "      <th>text_standard</th>\n",
       "      <th>mean_parse_tree_depth</th>\n",
       "      <th>mean_ents_per_sentence</th>\n",
       "      <th>mean_adj</th>\n",
       "      <th>mean_adp</th>\n",
       "      <th>mean_adv</th>\n",
       "      <th>mean_aux</th>\n",
       "      <th>mean_conj</th>\n",
       "      <th>mean_cconj</th>\n",
       "      <th>mean_det</th>\n",
       "      <th>mean_intj</th>\n",
       "      <th>mean_noun</th>\n",
       "      <th>mean_num</th>\n",
       "      <th>mean_part</th>\n",
       "      <th>mean_pron</th>\n",
       "      <th>mean_propn</th>\n",
       "      <th>mean_punct</th>\n",
       "      <th>mean_sconj</th>\n",
       "      <th>mean_sym</th>\n",
       "      <th>mean_verb</th>\n",
       "      <th>mean_x</th>\n",
       "      <th>mean_space</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>85.99</td>\n",
       "      <td>3.08</td>\n",
       "      <td>15.4</td>\n",
       "      <td>7.2</td>\n",
       "      <td>3.9</td>\n",
       "      <td>6.48</td>\n",
       "      <td>4.7</td>\n",
       "      <td>6.27</td>\n",
       "      <td>0.084661</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.39</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.926061</td>\n",
       "      <td>0.441441</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.03</td>\n",
       "      <td>1.84</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.47</td>\n",
       "      <td>2.04</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   flesch_reading_ease  spache_readability  mcalpine_eflaw  smog_index  \\\n",
       "0                85.99                3.08            15.4         7.2   \n",
       "\n",
       "   flesch_kincaid_grade  coleman_liau_index  automated_readability_index  \\\n",
       "0                   3.9                6.48                          4.7   \n",
       "\n",
       "   dale_chall_readability_score  difficult_words  linsear_write_formula  \\\n",
       "0                          6.27         0.084661                    5.0   \n",
       "\n",
       "   gunning_fog  text_standard  mean_parse_tree_depth  mean_ents_per_sentence  \\\n",
       "0         5.39            5.0               0.926061                0.441441   \n",
       "\n",
       "   mean_adj  mean_adp  mean_adv  mean_aux  mean_conj  mean_cconj  mean_det  \\\n",
       "0      0.48      0.97       0.6      0.86          0        0.41      0.74   \n",
       "\n",
       "   mean_intj  mean_noun  mean_num  mean_part  mean_pron  mean_propn  \\\n",
       "0       0.03       1.84      0.03       0.47       2.04        0.38   \n",
       "\n",
       "   mean_punct  mean_sconj  mean_sym  mean_verb  mean_x  mean_space  \n",
       "0        0.14        0.32         0       1.91       0         1.0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9123368b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['A2']], dtype=object)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catboost_model.predict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed150d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c827c29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cbe5ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e910f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e30bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ea55e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0c9fc7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import PyPDF2\n",
    "\n",
    "# creating a pdf reader object\n",
    "\n",
    "\n",
    "# print the number of pages in pdf file\n",
    "def get_sl(reader):\n",
    "    # print the text of the first page\n",
    "\n",
    "    def create_dataset_with_levels(word_list):\n",
    "        dataset = []\n",
    "        levels = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']\n",
    "        current_level = None\n",
    "        current_level_words = []\n",
    "        sl = {}\n",
    "        for word in word_list:\n",
    "            if word in levels:\n",
    "                # Found a level marker\n",
    "                current_level = word\n",
    "                current_level_words = []\n",
    "                sl[word] = []\n",
    "            elif word not in levels and '.' not in word and ',' not in word and current_level:\n",
    "                # Associate the word with the current level\n",
    "                sl[current_level].append(word)\n",
    "\n",
    "        return sl\n",
    "    \n",
    "    \n",
    "    \n",
    "    l = []\n",
    "    for i in range(len(reader.pages)):\n",
    "        l.append(reader.pages[i].extract_text().split())\n",
    "\n",
    "    m = []\n",
    "    for i in range(len(l)):\n",
    "        for j in range(len(l[i])):\n",
    "            m.append(l[i][j])\n",
    "\n",
    "            \n",
    "    return create_dataset_with_levels(m)\n",
    "\n",
    "\n",
    "\n",
    "reader = PyPDF2.PdfReader('Oxford_CEFR_level/American_Oxford_3000_by_CEFR_level.pdf')\n",
    "\n",
    "d1 = get_sl(reader)\n",
    "\n",
    "reader = PyPDF2.PdfReader('Oxford_CEFR_level/American_Oxford_5000_by_CEFR_level.pdf')\n",
    "\n",
    "d2 = get_sl(reader)\n",
    "\n",
    "\n",
    "reader = PyPDF2.PdfReader('Oxford_CEFR_level/The_Oxford_3000_by_CEFR_level.pdf')\n",
    "\n",
    "d3 = get_sl(reader)\n",
    "\n",
    "\n",
    "reader = PyPDF2.PdfReader('Oxford_CEFR_level/The_Oxford_5000_by_CEFR_level.pdf')\n",
    "\n",
    "\n",
    "d4 = get_sl(reader)\n",
    "dictm = {}\n",
    "\n",
    "for d in [d1, d2, d3, d4]:\n",
    "    for key, value in d.items():\n",
    "        if key in dictm:\n",
    "            dictm[key].extend(value)  # Если ключ уже есть в объединенном словаре, просто добавляем значения из текущего словаря\n",
    "        else:\n",
    "            dictm[key] = value[:]  # Если ключа еще нет, создаем новую запись в объединенном словаре\n",
    "\n",
    "\n",
    "\n",
    "english_words_c2 = [\n",
    "    \"Aberration\", \"Abhorrent\", \"Abject\", \"Abscond\", \"Abstruse\", \"Accolade\", \"Acerbic\", \"Acquiesce\", \"Adamant\",\n",
    "    \"Adulation\", \"Aesthetic\", \"Alacrity\", \"Alleviate\", \"Amalgamate\", \"Ambiguous\", \"Ameliorate\", \"Anachronism\",\n",
    "    \"Anathema\", \"Animosity\", \"Antithesis\", \"Apotheosis\", \"Apprehensive\", \"Arcane\", \"Assiduous\", \"Assuage\", \"Audacious\",\n",
    "    \"Auspicious\", \"Bellicose\", \"Benevolent\", \"Bombastic\", \"Cacophony\", \"Callous\", \"Capricious\", \"Cathartic\",\n",
    "    \"Clandestine\", \"Coalesce\", \"Complacent\", \"Concomitant\", \"Confluence\", \"Conjecture\", \"Contrite\", \"Culpable\",\n",
    "    \"Deleterious\", \"Despondent\", \"Diatribe\", \"Disparage\", \"Ebullient\", \"Ephemeral\", \"Epiphany\", \"Esoteric\", \"Euphoria\",\n",
    "    \"Exacerbate\", \"Exhort\", \"Exonerate\", \"Facetious\", \"Fecund\", \"Garrulous\", \"Gregarious\", \"Hackneyed\", \"Harbinger\",\n",
    "    \"Iconoclast\", \"Idiosyncratic\", \"Impetuous\", \"Ineffable\", \"Inscrutable\", \"Intrepid\", \"Intractable\", \"Inveterate\",\n",
    "    \"Juxtapose\", \"Languid\", \"Loquacious\", \"Magnanimous\", \"Mellifluous\", \"Mendacious\", \"Mercurial\", \"Munificent\",\n",
    "    \"Nefarious\", \"Nostalgia\", \"Obfuscate\", \"Obtuse\", \"Onerous\", \"Palliate\", \"Panacea\", \"Pariah\", \"Perfidious\",\n",
    "    \"Perspicacious\", \"Platitude\", \"Pragmatic\", \"Prolific\", \"Pugnacious\", \"Querulous\", \"Quixotic\", \"Rancorous\",\n",
    "    \"Rapport\", \"Recalcitrant\", \"Redolent\", \"Replete\", \"Reticent\", \"Salubrious\", \"Sycophant\"\n",
    "]\n",
    "english_words_c2 += [\n",
    "    \"Sagacious\", \"Sanguine\", \"Solecism\", \"Soporific\", \"Specious\", \"Sycophant\", \"Trenchant\", \"Truncate\",\n",
    "    \"Ubiquitous\", \"Unctuous\", \"Untenable\", \"Vacillate\", \"Vehement\", \"Venerate\", \"Veracity\", \"Verbose\",\n",
    "    \"Vexation\", \"Vilify\", \"Voracious\", \"Wanderlust\", \"Xenophobia\", \"Yoke\", \"Zephyr\", \"Zealous\"\n",
    "]\n",
    "\n",
    "english_words_c2 += [\n",
    "    \"Absolution\", \"Accretion\", \"Belligerent\", \"Cacophonous\", \"Debilitate\", \"Ebullience\", \"Exacerbation\",\n",
    "    \"Facilitate\", \"Garrulity\", \"Hapless\", \"Iconoclastic\", \"Juxtaposition\", \"Kaleidoscope\", \"Labyrinthine\",\n",
    "    \"Meticulous\", \"Nebulous\", \"Obfuscation\", \"Panegyric\", \"Pernicious\", \"Quintessential\", \"Recalcitrance\",\n",
    "    \"Sycophantic\", \"Transcendence\", \"Ubiquity\", \"Vacuous\", \"Wistful\", \"Xenial\", \"Yield\", \"Zealotry\"\n",
    "]\n",
    "english_words_c2 += [\n",
    "    \"Acrimonious\", \"Ambivalent\", \"Benevolence\", \"Capitulate\", \"Clandestine\", \"Cacophony\", \"Debunk\", \"Deleterious\",\n",
    "    \"Disparate\", \"Elucidate\", \"Ephemeral\", \"Esoteric\", \"Euphemism\", \"Exacerbate\", \"Extemporaneous\", \"Fecund\",\n",
    "    \"Fortuitous\", \"Gregarious\", \"Hackneyed\", \"Hapless\", \"Iconoclast\", \"Idiosyncratic\", \"Impetuous\", \"Ineffable\",\n",
    "    \"Ingratiate\", \"Insidious\", \"Juxtapose\", \"Laconic\", \"Languid\", \"Mellifluous\", \"Mendacious\", \"Nefarious\",\n",
    "    \"Ostentatious\", \"Palpable\", \"Perfidious\", \"Pernicious\", \"Platitude\", \"Pragmatic\", \"Quintessential\", \"Rancorous\",\n",
    "    \"Sagacious\", \"Sanguine\", \"Sycophantic\", \"Trenchant\", \"Ubiquitous\", \"Untenable\", \"Vacillate\", \"Vehement\",\n",
    "    \"Veracity\", \"Verbose\", \"Vexation\", \"Vilify\", \"Wanderlust\", \"Xenophobia\", \"Yoke\", \"Zephyr\", \"Zealous\",\n",
    "    \"Absolution\", \"Accretion\", \"Belligerent\", \"Cacophonous\", \"Debilitate\", \"Ebullience\", \"Exacerbation\",\n",
    "    \"Facilitate\", \"Garrulity\", \"Hapless\", \"Iconoclastic\", \"Juxtaposition\", \"Kaleidoscope\", \"Labyrinthine\",\n",
    "    \"Meticulous\", \"Nebulous\", \"Obfuscation\", \"Panegyric\", \"Pernicious\", \"Quintessential\", \"Recalcitrance\",\n",
    "    \"Sycophantic\", \"Transcendence\", \"Ubiquity\", \"Vacuous\", \"Wistful\", \"Xenial\", \"Yield\", \"Zealotry\",\n",
    "    \"Aberrant\", \"Abhor\", \"Accolades\", \"Acrimony\", \"Adamantly\", \"Adversity\", \"Aesthetics\", \"Alacrity\",\n",
    "    \"Alleviation\", \"Altruism\", \"Amicable\", \"Anomaly\", \"Antipathy\", \"Arduous\", \"Articulate\", \"Assimilate\",\n",
    "    \"Audacity\", \"Augment\", \"Auspice\", \"Belligerence\", \"Blatant\", \"Bolster\", \"Cacophonic\", \"Camaraderie\",\n",
    "    \"Capriciousness\", \"Catharsis\", \"Cogent\", \"Collaborative\", \"Comprehensive\", \"Concordant\", \"Conscientious\",\n",
    "    \"Construe\", \"Contentious\", \"Contrition\", \"Convivial\", \"Cursory\", \"Debilitated\", \"Defunct\", \"Deleteriously\",\n",
    "    \"Despondency\", \"Diligent\", \"Disconcerting\", \"Disparaging\", \"Duplicity\", \"Edification\", \"Effervescent\",\n",
    "    \"Efficacious\", \"Eloquent\", \"Embellish\", \"Empathy\", \"Endeavor\", \"Enigmatic\", \"Equanimity\", \"Eradicate\",\n",
    "    \"Erroneous\", \"Ethereal\", \"Exacerbating\", \"Exasperate\", \"Exemplary\", \"Expedient\", \"Expunge\", \"Extol\",\n",
    "    \"Facilitation\", \"Fervent\", \"Flourish\", \"Fortitude\", \"Frivolous\", \"Garrulousness\", \"Gratuitous\", \"Guileless\",\n",
    "    \"Harangue\", \"Heretic\", \"Hyperbole\", \"Hypothetical\", \"Immutable\", \"Impassioned\", \"Impeccable\", \"Imperturbable\",\n",
    "    \"Inception\", \"Incorrigible\", \"Indifferent\", \"Inefficacious\", \"Infallible\", \"Ingenuity\", \"Innovative\", \"Insatiable\",\n",
    "    \"Insipid\", \"Intrepidly\", \"Inundate\", \"Juxtaposed\", \"Kinetic\", \"Lamentation\", \"Laudatory\", \"Loquaciousness\",\n",
    "    \"Lucid\", \"Magniloquent\", \"Malevolent\", \"Mellifluously\", \"Mendacity\", \"Meticulousness\", \"Mitigate\", \"Munificence\",\n",
    "    \"Nefariously\", \"Nostalgic\", \"Ominous\", \"Opulent\", \"Panacea\", \"Pensive\", \"Perfunctory\", \"Phenomenon\",\n",
    "    \"Plethora\", \"Ponderous\", \"Pristine\", \"Proclivity\", \"Prolificacy\", \"Propensity\", \"Prudent\", \"Puerile\",\n",
    "    \"Quintessentially\", \"Quotidian\", \"Recalcitrant\", \"Redundant\", \"Replete\", \"Resilient\", \"Reticence\", \"Rhapsody\",\n",
    "    \"Sagacity\", \"Sanguineous\", \"Sardonic\", \"Serenity\", \"Sinister\", \"Sophistry\", \"Soporifically\", \"Spurious\",\n",
    "    \"Stagnant\", \"Sublime\", \"Surreptitious\", \"Sycophantically\", \"Taciturn\", \"Tantalizing\", \"Tempestuous\", \"Tendentious\",\n",
    "    \"Tenuous\", \"Trepidation\", \"Ubiquitously\", \"Unassailable\", \"Unprecedented\", \"Unrequited\", \"Unscrupulous\", \"Untoward\",\n",
    "    \"Uproarious\", \"Vacillation\", \"Vehemently\", \"Veritable\", \"Vexatious\", \"Vilification\", \"Vociferous\", \"Voraciously\",\n",
    "    \"Wane\", \"Waver\", \"Xenophobia\", \"Yieldingly\", \"Zealously\", \"Zenith\"\n",
    "]\n",
    "\n",
    "english_words_c2 += [\n",
    "    \"Aberrations\", \"Abhorrence\", \"Abjectly\", \"Absconding\", \"Abstruseness\", \"Acclamatory\", \"Acrimoniously\", \"Adamantness\",\n",
    "    \"Adulations\", \"Aestheticism\", \"Alacritously\", \"Alleviations\", \"Amalgamations\", \"Ambiguities\", \"Ameliorations\",\n",
    "    \"Anachronistic\", \"Anathematize\", \"Animosities\", \"Antagonistic\", \"Antipathies\", \"Aphoristically\", \"Apostates\",\n",
    "    \"Approbation\", \"Archetypical\", \"Arduousness\", \"Articulately\", \"Assiduously\", \"Assuagements\", \"Auspiciously\",\n",
    "    \"Belligerently\", \"Bellicosity\", \"Beneficently\", \"Bombastically\", \"Cacophonously\", \"Camaraderie\", \"Capitulation\",\n",
    "    \"Clandestinely\", \"Collaboration\", \"Complaisantly\", \"Conciliatory\", \"Concordantly\", \"Conscientiousness\",\n",
    "    \"Constrictive\", \"Contemporaneous\", \"Contiguously\", \"Contradiction\", \"Contrariness\", \"Convalescence\",\n",
    "    \"Conventional\", \"Cosmopolitan\", \"Culpability\", \"Debilitating\", \"Debilitatingly\", \"Debunking\", \"Decorousness\",\n",
    "    \"Decrepitude\", \"Defenestrate\", \"Deleteriously\", \"Deliberation\", \"Despondently\", \"Detestable\", \"Deviousness\",\n",
    "    \"Diaphanous\", \"Didacticism\", \"Dilettantism\", \"Disaffection\", \"Disappointment\", \"Disparagements\", \"Dissonantly\",\n",
    "    \"Dissolution\", \"Duplicities\", \"Edaciousness\", \"Edification\", \"Effervescently\", \"Efficaciously\", \"Effrontery\",\n",
    "    \"Elaborations\", \"Elatedness\", \"Elucidation\", \"Embittered\", \"Emendations\", \"Encompassing\", \"Enervation\",\n",
    "    \"Engrossingly\", \"Enigmaticness\", \"Equitableness\", \"Eradication\", \"Erroneously\", \"Estrangement\", \"Ethnological\",\n",
    "    \"Euphemistically\", \"Exacerbated\", \"Exacerbatingly\", \"Exasperation\", \"Exemplification\", \"Exhaustiveness\",\n",
    "    \"Exorbitantly\", \"Expediencies\", \"Expediently\", \"Expeditiously\", \"Exploitative\", \"Expurgation\", \"Extemporizing\",\n",
    "    \"Facetiousness\", \"Factitiousness\", \"Facultatively\", \"Fallaciousness\", \"Fascinatingly\", \"Fecklessness\",\n",
    "    \"Felicitation\", \"Fictitiously\", \"Flamboyantly\", \"Flauntingly\", \"Fluctuations\", \"Fortification\", \"Fortuitously\",\n",
    "    \"Fulsomely\", \"Furtiveness\", \"Gallivanting\", \"Galvanizing\", \"Garishly\", \"Generosities\", \"Gratuitously\",\n",
    "    \"Gregariously\", \"Hallowedness\", \"Harmoniously\", \"Harrowingly\", \"Histrionically\", \"Horticultural\",\n",
    "    \"Hubristically\", \"Hypertrophied\", \"Hypnotically\", \"Idyllically\", \"Illegitimate\", \"Illustriously\", \"Imbecilic\",\n",
    "    \"Immaculately\", \"Immethodical\", \"Immovability\", \"Impassiveness\", \"Impersonally\", \"Impracticably\", \"Impromptu\",\n",
    "    \"Impudence\", \"Inappreciable\", \"Incapacitated\", \"Incessantly\", \"Incorrigibility\", \"Incredulously\", \"Incumbently\",\n",
    "    \"Indefatigably\", \"Inefficaciously\", \"Inexorability\", \"Inexpugnable\", \"Infallibility\", \"Informatively\", \"Inimically\",\n",
    "    \"Inopportunely\", \"Inscrutableness\", \"Insinuatingly\", \"Insipidness\", \"Insolubility\", \"Insolubly\", \"Instructively\",\n",
    "    \"Insufferable\", \"Insupportably\", \"Intempestively\", \"Intentionality\", \"Interchangeably\", \"Interconnections\",\n",
    "    \"Intergalactic\", \"Interminability\", \"Intimidation\", \"Intoxicating\", \"Intractability\", \"Intrepidness\", \"Intriguingly\",\n",
    "    \"Inviolate\", \"Involuntarily\", \"Invulnerability\", \"Irrefutably\", \"Irresistible\", \"Jingoistically\", \"Joviality\",\n",
    "    \"Judiciously\", \"Juxtapositions\", \"Kaleidoscopic\", \"Laconically\", \"Languorously\", \"Lapidary\", \"Lavishness\",\n",
    "    \"Legitimately\", \"Levity\", \"Libertarian\", \"Lilliputian\", \"Lionhearted\", \"Loquaciousness\", \"Luminescent\",\n",
    "    \"Lyricalness\", \"Machiavellian\", \"Mafioso\", \"Magnanimously\", \"Magniloquence\", \"Maladroit\", \"Malleability\",\n",
    "    \"Manifestation\", \"Manifoldly\", \"Mannerliness\", \"Marvellously\", \"Mawkishness\", \"Meanderingly\", \"Melancholically\",\n",
    "    \"Mellifluousness\", \"Memorability\", \"Mendaciously\", \"Mendicancy\", \"Metamorphosis\", \"Metaphorical\", \"Meticulousness\",\n",
    "    \"Microcosmic\", \"Milquetoast\", \"Minacious\", \"Ministration\", \"Miraculousness\", \"Mitigating\", \"Moderateness\",\n",
    "    \"Momentousness\", \"Monotonically\", \"Multifariously\", \"Multivalence\", \"Multitudinously\", \"Munificently\",\n",
    "    \"Mutualities\", \"Myriad\", \"Mystification\", \"Mythological\", \"Nascent\", \"Nefariously\", \"Negotiability\",\n",
    "    \"Neuroticism\", \"Neutralization\", \"Nimbleness\", \"Nomenclature\", \"Nonchalantly\", \"Nonetheless\", \"Notoriously\",\n",
    "    \"Nugatory\", \"Nurturance\", \"Obfuscatingly\", \"Obstreperous\", \"Obtrusively\", \"Odiousness\", \"Officiousness\",\n",
    "    \"Oligarchic\", \"Onerously\", \"Opalescent\", \"Operability\", \"Opportunely\", \"Opprobriously\", \"Optimization\",\n",
    "    \"Ornamentally\", \"Ornithology\", \"Orthographical\", \"Orthographically\", \"Ostentatiously\", \"Outlandishly\",\n",
    "    \"Outspokenness\", \"Overambitious\", \"Overestimations\", \"Overindulgence\", \"Overweening\", \"Oxymoronic\",\n",
    "    \"Palatability\", \"Palliative\", \"Panegyrical\"]\n",
    "\n",
    "english_words_c2 += [\n",
    "    \"Paradoxically\", \"Parochialism\", \"Particularity\", \"Pathological\", \"Pellucidly\", \"Penetrative\", \"Penumbral\",\n",
    "    \"Perambulate\", \"Perambulation\", \"Perdurability\", \"Peremptorily\", \"Perfidiously\", \"Perfunctorily\", \"Peripatetic\",\n",
    "    \"Periphrastic\", \"Perpetrator\", \"Perplexingly\", \"Persnickety\", \"Personification\", \"Perspicuously\", \"Pertinacious\",\n",
    "    \"Pertinently\", \"Pervicacious\", \"Petrichor\", \"Phenomenally\", \"Philanthropic\", \"Philistinism\", \"Phlegmatic\",\n",
    "    \"Piquant\", \"Plenipotentiary\", \"Plenitude\", \"Plenitudinous\", \"Plenteous\", \"Polyglot\", \"Polyphony\", \"Polyvalent\",\n",
    "    \"Positivistic\", \"Posthumous\", \"Postprandial\", \"Potentate\", \"Pragmatism\", \"Pragmatist\", \"Precipitancy\",\n",
    "    \"Predestination\", \"Preposterously\", \"Prescient\", \"Presumptuous\", \"Pretentiously\", \"Pristinely\", \"Probity\",\n",
    "    \"Procrastination\", \"Profoundly\", \"Profundity\", \"Progenitor\", \"Prognostication\", \"Prohibitive\", \"Propinquity\",\n",
    "    \"Propitiation\", \"Prosaically\", \"Prosperously\", \"Prostration\", \"Protuberance\", \"Prowess\", \"Prudential\",\n",
    "    \"Punctiliousness\", \"Punctuality\", \"Puniness\", \"Pyrrhic\", \"Quandary\", \"Quintessence\", \"Quizzically\", \"Raconteur\",\n",
    "    \"Rambunctious\", \"Rapidity\", \"Rapscallion\", \"Ratification\", \"Recapitulation\", \"Recondite\", \"Recrimination\",\n",
    "    \"Redemptive\", \"Redolence\", \"Refulgence\", \"Refutation\", \"Regurgitate\", \"Rejuvenescence\", \"Rejuvenation\",\n",
    "    \"Relativism\", \"Relinquishment\", \"Remonstrance\", \"Renaissance\", \"Reparations\", \"Repertoire\", \"Replete\",\n",
    "    \"Reprehensibility\", \"Reproachfully\", \"Reproof\", \"Reprove\", \"Repugnant\", \"Requiem\", \"Rescind\", \"Resonant\",\n",
    "    \"Resplendent\", \"Restitution\", \"Resurgence\", \"Reticulation\", \"Reticulocytosis\", \"Retrogression\", \"Revivify\",\n",
    "    \"Reviviscence\", \"Revulsion\", \"Rhapsodical\", \"Ribald\", \"Rigorousness\", \"Riposte\", \"Ritualistic\", \"Robustious\",\n",
    "    \"Ruminative\", \"Rutherfordium\", \"Sacrosanct\", \"Sagaciousness\", \"Salubriously\", \"Salutary\", \"Salutatorian\",\n",
    "    \"Sanctimonious\", \"Sanguinity\", \"Sapiential\", \"Saprophytic\", \"Sardonically\", \"Saturnine\", \"Scintillation\",\n",
    "    \"Scrupulousness\", \"Scurrilous\", \"Sequester\", \"Serendipitous\", \"Sereneness\", \"Sesquipedalian\", \"Sibylline\",\n",
    "    \"Simpatico\", \"Simultaneity\", \"Sinecure\", \"Singularity\", \"Sinisterly\", \"Sobriquet\", \"Solicitous\", \"Solicitude\",\n",
    "    \"Soliloquy\", \"Solipsism\", \"Soporiferous\", \"Spectroscopy\", \"Spectroscopic\", \"Spectacularly\", \"Splenetic\",\n",
    "    \"Splendiferous\", \"Spontaneity\", \"Sprightliness\", \"Stalwartness\", \"Staunchness\", \"Steadfastness\", \"Stentorian\",\n",
    "    \"Stigmatization\", \"Stimulative\", \"Stipendiary\", \"Stipulation\", \"Stoicism\", \"Stratagem\", \"Stultifying\",\n",
    "    \"Subconsciousness\", \"Substantive\", \"Subterfuge\", \"Subterranean\", \"Sufficiency\", \"Suggestiveness\", \"Supercilious\",\n",
    "    \"Superficiality\", \"Supernumerary\", \"Supersede\", \"Supersession\", \"Supervenient\", \"Supposititious\", \"Surfeited\",\n",
    "    \"Surreptitiously\", \"Surreptitiousness\", \"Sustentation\", \"Sycophantically\", \"Sycophantish\", \"Symphonic\", \"Synchronicity\",\n",
    "    \"Synchronism\", \"Tactician\", \"Tactility\", \"Taciturnity\", \"Tactfulness\", \"Tantamount\", \"Tarradiddle\", \"Tatterdemalion\",\n",
    "    \"Teetotaler\", \"Tendentiousness\", \"Terpsichorean\", \"Terrestrial\", \"Territoriality\", \"Thaumaturgy\", \"Theatricality\",\n",
    "    \"Theocracy\", \"Theosophy\", \"Threnody\", \"Thrivingly\", \"Tidiness\", \"Tigerish\", \"Tirade\", \"Titillation\", \"Toponym\",\n",
    "    \"Torchbearer\", \"Touchiness\", \"Traducement\", \"Transcendental\", \"Transcendentally\", \"Transfiguration\", \"Transmutation\",\n",
    "    \"Travesty\", \"Treacherously\", \"Tremendously\", \"Trenchantly\", \"Triangulation\", \"Tribulation\", \"Trifurcation\", \"Triskaidekaphobia\",\n",
    "    \"Truculence\", \"Trumpery\", \"Trustworthiness\", \"Turgidness\", \"Turbidity\", \"Turpitude\", \"Ubiquitously\", \"Umbrage\", \"Unassumingness\",\n",
    "    \"Unattainable\", \"Unavailing\", \"Unbending\", \"Unconscionable\", \"Uncontroversial\", \"Unconventionality\", \"Uncorrupted\",\n",
    "    \"Undeniably\", \"Underling\", \"Underpinning\", \"Underscore\", \"Undulate\", \"Unencumbered\", \"Unequivocal\", \"Unexceptionable\",\n",
    "    \"Unflappable\", \"Unfounded\", \"Unhallowed\", \"Unimpeachable\", \"Unimpeachably\", \"Uninhibitedly\", \"Uninhibitedness\", \"Uninvolved\",\n",
    "    \"Universalism\", \"Unmitigated\", \"Unobstructed\", \"Unpalatable\", \"Unpretentious\", \"Unprepossessing\", \"Unpropitious\", \"Unqualified\",\n",
    "    \"Unremitting\", \"Unsparing\", \"Unswerving\", \"Untarnished\", \"Untiring\", \"Unwavering\", \"Uprightness\", \"Uproariously\", \"Uproariousness\",\n",
    "    \"Uprooted\", \"Uprootment\", \"Usurp\", \"Usurper\", \"Utilitarianism\", \"Uxorious\", \"Vacillant\", \"Vacillation\", \"Vacuousness\", \"Vainglorious\",\n",
    "    \"Valediction\", \"Valedictorian\", \"Valetudinarian\", \"Vehemency\", \"Vegetarianism\", \"Venerable\", \"Venial\", \"Venturous\", \"Veracious\",\n",
    "    \"Verdancy\", \"Verifiably\", \"Verisimilitude\", \"Vernacular\", \"Verterbrate\", \"Vertiginous\", \"Vicarious\", \"Vicissitudes\", \"Vilipend\",\n",
    "    \"Vindication\", \"Virtuosity\", \"Virtuousness\", \"Virulently\", \"Visceral\", \"Vitriolic\", \"Vituperate\", \"Vituperative\", \"Vivaciousness\",\n",
    "    \"Vivacity\", \"Vociferousness\", \"Voluptuous\", \"Voraciousness\", \"Voyeuristic\", \"Vulcanization\", \"Vulnerary\", \"Vulnerable\", \"Waggishness\",\n",
    "    \"Wardship\", \"Warmongering\", \"Warrantable\", \"Warranted\", \"Wealthiness\", \"Welterweight\", \"Whimsically\", \"Whimsicalness\", \"Wholesomeness\",\n",
    "    \"Wholesale\", \"Wistfulness\", \"Witless\", \"Witticism\", \"Wittily\", \"Woebegone\", \"Wonderstruck\", \"Wonted\", \"Wooingly\", \"Woolliness\",\n",
    "    \"Worshipfulness\", \"Wretchedness\", \"Wrigglingly\", \"Xenogeneic\", \"Xenophilic\", \"Xerophilous\", \"Yesteryear\", \"Yieldingness\", \"Youthful\",\n",
    "    \"Zephyrous\", \"Zeugma\", \"Zestful\", \"Zoophagous\"\n",
    "]\n",
    "english_words_c2 = [word.lower() for word in english_words_c2]\n",
    "\n",
    "\n",
    "\n",
    "dictm['C2'] = english_words_c2\n",
    "for key in dictm.keys():\n",
    "    dictm[key] = list(set(dictm[key]))\n",
    "# исходный словарь\n",
    "original_dict = dictm\n",
    "\n",
    "# Создаем новый упорядоченный словарь, основанный на исходном\n",
    "ordered_dict = OrderedDict(original_dict)\n",
    "\n",
    "# Меняем порядок ключей вручную\n",
    "new_order = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']\n",
    "level_dict = OrderedDict((key, ordered_dict[key]) for key in new_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 955,
   "id": "c7ab3fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['predicted_level'] = df['Subtitles'].apply(f, o_dict = level_dict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 956,
   "id": "5137de0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Level</th>\n",
       "      <th>Subtitles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B1</td>\n",
       "      <td>fix &amp; sync bozxphd enjoy flick clang drawer cl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B1</td>\n",
       "      <td>hey ill right , cameron go nine school year , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B2</td>\n",
       "      <td>resync xenzainef retail help he due list two m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B2</td>\n",
       "      <td>sync correct mrcjnthn get black eye open wide ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A2</td>\n",
       "      <td>oh , come land faraway place caravan camel roa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1729</th>\n",
       "      <td>C2</td>\n",
       "      <td>seaworld suffer collaps profit custom desert c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1730</th>\n",
       "      <td>C2</td>\n",
       "      <td>may know steve job even tie shoelac , averag s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>C2</td>\n",
       "      <td>long time ago , cinema audienc transport galax...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1732</th>\n",
       "      <td>C2</td>\n",
       "      <td>may yet live age fli car , predict film back f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>C2</td>\n",
       "      <td>set asid epic disastermovi moment volcano , hu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1734 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Level                                          Subtitles\n",
       "0       B1  fix & sync bozxphd enjoy flick clang drawer cl...\n",
       "1       B1  hey ill right , cameron go nine school year , ...\n",
       "2       B2  resync xenzainef retail help he due list two m...\n",
       "3       B2  sync correct mrcjnthn get black eye open wide ...\n",
       "4       A2  oh , come land faraway place caravan camel roa...\n",
       "...    ...                                                ...\n",
       "1729    C2  seaworld suffer collaps profit custom desert c...\n",
       "1730    C2  may know steve job even tie shoelac , averag s...\n",
       "1731    C2  long time ago , cinema audienc transport galax...\n",
       "1732    C2  may yet live age fli car , predict film back f...\n",
       "1733    C2  set asid epic disastermovi moment volcano , hu...\n",
       "\n",
       "[1734 rows x 2 columns]"
      ]
     },
     "execution_count": 956,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0905aff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(text, o_dict):\n",
    "    \n",
    "    # Preprocess the text (if needed)\n",
    "    # You can use the same preprocess_text function mentioned earlier, or customize it based on your requirements.\n",
    "    \n",
    "    text = text.replace('\\ufeff', '')\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[,:;()\\-]\", \" \", text)  # Override commas, colons, etc to spaces/\n",
    "    text = re.sub(r\"[\\.!?]\", \".\", text)  # Change all terminators like ! and ? to \".\"\n",
    "    text = re.sub(r\"^\\s+\", \"\", text)  # Remove white space\n",
    "    text = re.sub(r\"[ ]*(\\n|\\r\\n|\\r)[ ]*\", \" \", text)  # Remove new lines\n",
    "    text = re.sub(r'\\d+:\\d+:\\d+,\\d+|\\d+|<[^>]*>|-|–|>|%|[’\\':;♪(){}\\[\\]]', '', text)  # Remove timecodes, digits, HTML tags, special characters, brackets, exclamation marks, question marks, colons, and other similar characters\n",
    "    text = re.sub(r\"([\\.])[\\. ]+\", \".\", text)  # Change all \"..\" to \".\"\n",
    "    text = re.sub(r\"[ ]*([\\.])\", \". \", text)  # Normalize all \".\"`\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces\n",
    "    text = re.sub(r\"\\s+$\", \"\", text)  # Remove trailing spaces\n",
    "    text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text)\n",
    "    text = text.replace('\"', '')  # Remove smart quotes\n",
    "    text = ''.join(char for char in text if ord(char) < 128)\n",
    "    text = text.replace('\\ufeff', '')\n",
    "    text = text.replace('.', '')\n",
    "    \n",
    "    \n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    # Join the processed words back into a single string\n",
    "    text = ' '.join(words)\n",
    "\n",
    "    \n",
    "    \n",
    "#     stemmer = PorterStemmer()\n",
    "    \n",
    "#     # Tokenize the text into individual words\n",
    "#     words = word_tokenize(text)\n",
    "    \n",
    "#     # Apply stemming to each word and store the results in a list\n",
    "#     stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "#     # Join the stemmed words back into a single string\n",
    "#     text = ' '.join(stemmed_words)\n",
    "    \n",
    "    \n",
    "#     words = word_tokenize(text)\n",
    "#     lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "#     lemmed_words = [lemma.lemmatize(word) for word in words]\n",
    "#     text = ' '.join(lemmed_words)\n",
    "    \n",
    "    \n",
    "    # Tokenize the text into individual words\n",
    "    words = text.split()\n",
    "    words = [word.lower() for word in words]\n",
    "#     print(words)\n",
    "    # Initialize a dictionary to store the sum of weights for each difficulty level\n",
    "    difficulty_weight_sum = {level: 0 for level in o_dict.keys()}\n",
    "    \n",
    "    weights = {'A1':1, 'A2':1, 'B1':1, 'B2':1, 'C1':1, 'C2':1}  \n",
    "    \n",
    "    # Calculate the sum of weights for each difficulty level in the text\n",
    "    for word in words:\n",
    "        for level, word_list in o_dict.items():\n",
    "            if word in word_list:\n",
    "                # Assign the weight as the difficulty level (higher level -> bigger weight)\n",
    "#                 weight = int(level[1:])\n",
    "                difficulty_weight_sum[level] += weights[level]\n",
    "#             else:\n",
    "#                 g = 1\n",
    "#                 for level in o_dict.keys():\n",
    "#                     difficulty_weight_sum[level] += g\n",
    "#                     g+=1\n",
    "    # Normalize the weights to obtain the final difficulty level probabilities\n",
    "    total_weight = sum(difficulty_weight_sum.values())\n",
    "    features = {level: (weight_sum / total_weight) + 1 for level, weight_sum in difficulty_weight_sum.items()}\n",
    "    return list(features.values())\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90052b8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8341311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166d005c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9484276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b31436b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d625e140",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd5d42c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178c9749",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbd3a63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2390b407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94dd73d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca9ef51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6873b01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f6dc24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc1fe96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1920c45c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bac4240",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e2e68f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0f26ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d27069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef6cda0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d9d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae36ffdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72cd2775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30a0afb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667f4377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11a1e58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a755d6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3478b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba650e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee0d4dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa12ab2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c455cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26a41d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a8c1d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d39df64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d64549a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d93b149",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993bf0b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8155a601",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd345f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ffe1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf1c7a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a768b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc541119",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c31248",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7341638",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5bb0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2e95dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45500c73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f6cb2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6c40a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
